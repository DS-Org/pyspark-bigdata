{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from spark 2.0 sparkSession is being used instead of sparkContext\n",
    "earlier sparkContext came with RDD api support and for sql we need sqlcontext and for streaming we need streaming context. but with SparkSession, it can be treated as single point of entry for all the spark features\n",
    "\n",
    "SparkSession also had support for datasets and rows API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Friends\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads as DF directly\n",
    "data = spark.read.text(\"./datasets/fakefriends.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipelinedRDD is a subset of RDD and is returned when a map or any transformation is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when to use cache, \n",
    "#if an action is called on RDD repetatively or couple of action, its better to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an RDD and then convert into dF later\n",
    "data = spark.sparkContext.textFile(\"./datasets/fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us go ahead and create a RDD with rows and add some structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowMapper(line):\n",
    "    fields = line.split(\",\")\n",
    "    return Row(ID=int(fields[0]), \n",
    "               name = fields[1],\n",
    "               age = int(fields[2]),\n",
    "               numOfFriends = int(fields[3])\n",
    "              )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_data = data.map(rowMapper) # [Row(ID=0, age=33, name=b'Will', numOfFriends=385)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* conversion of RDD to df needs to be cached as this is an expensive operation\n",
    "* when cache is used it doesnt mean the data is read from file\n",
    "* it will cache when an action is called amd file is read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFER SCHEMA\n",
    "schemaFriends = spark.createDataFrame(friends_data).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register df as tables\n",
    "schemaFriends.createOrReplaceTempView(\"friends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "teenagers = spark.sql(\"select * from friends where age >= 13 and age <=19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(teenagers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+------------+\n",
      "| ID|age|   name|numOfFriends|\n",
      "+---+---+-------+------------+\n",
      "| 21| 19|  Miles|         268|\n",
      "| 52| 19|Beverly|         269|\n",
      "| 54| 19|  Brunt|           5|\n",
      "|106| 18|Beverly|         499|\n",
      "|115| 18|  Dukat|         397|\n",
      "|133| 19|  Quark|         265|\n",
      "|136| 19|   Will|         335|\n",
      "|225| 19|   Elim|         106|\n",
      "|304| 19|   Will|         404|\n",
      "|341| 18|   Data|         326|\n",
      "|366| 19|  Keiko|         119|\n",
      "|373| 19|  Quark|         272|\n",
      "|377| 18|Beverly|         418|\n",
      "|404| 18| Kasidy|          24|\n",
      "|409| 19|    Nog|         267|\n",
      "|439| 18|   Data|         417|\n",
      "|444| 18|  Keiko|         472|\n",
      "|492| 19|  Dukat|          36|\n",
      "|494| 18| Kasidy|         194|\n",
      "+---+---+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=21, age=19, name='Miles', numOfFriends=268),\n",
       " Row(ID=52, age=19, name='Beverly', numOfFriends=269),\n",
       " Row(ID=54, age=19, name='Brunt', numOfFriends=5),\n",
       " Row(ID=106, age=18, name='Beverly', numOfFriends=499),\n",
       " Row(ID=115, age=18, name='Dukat', numOfFriends=397),\n",
       " Row(ID=133, age=19, name='Quark', numOfFriends=265),\n",
       " Row(ID=136, age=19, name='Will', numOfFriends=335),\n",
       " Row(ID=225, age=19, name='Elim', numOfFriends=106),\n",
       " Row(ID=304, age=19, name='Will', numOfFriends=404),\n",
       " Row(ID=341, age=18, name='Data', numOfFriends=326),\n",
       " Row(ID=366, age=19, name='Keiko', numOfFriends=119),\n",
       " Row(ID=373, age=19, name='Quark', numOfFriends=272),\n",
       " Row(ID=377, age=18, name='Beverly', numOfFriends=418),\n",
       " Row(ID=404, age=18, name='Kasidy', numOfFriends=24),\n",
       " Row(ID=409, age=19, name='Nog', numOfFriends=267),\n",
       " Row(ID=439, age=18, name='Data', numOfFriends=417),\n",
       " Row(ID=444, age=18, name='Keiko', numOfFriends=472),\n",
       " Row(ID=492, age=19, name='Dukat', numOfFriends=36),\n",
       " Row(ID=494, age=18, name='Kasidy', numOfFriends=194)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teenagers.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the same using spark methods\n",
    "\n",
    "![alt sql methods](./images/spark-sql-methods.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "| Kasidy| 18|\n",
      "|   Data| 18|\n",
      "| Kasidy| 18|\n",
      "|   Data| 18|\n",
      "|  Dukat| 18|\n",
      "|Beverly| 18|\n",
      "|Beverly| 18|\n",
      "|  Keiko| 18|\n",
      "|  Keiko| 19|\n",
      "|  Quark| 19|\n",
      "|    Nog| 19|\n",
      "|   Will| 19|\n",
      "|  Brunt| 19|\n",
      "|Beverly| 19|\n",
      "|  Quark| 19|\n",
      "|   Will| 19|\n",
      "|  Dukat| 19|\n",
      "|   Elim| 19|\n",
      "|  Miles| 19|\n",
      "|    Nog| 20|\n",
      "+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaFriends.select(\"name\",\"age\").filter((schemaFriends['age'] >=13) &\n",
    "                                (schemaFriends['age'] <=20)).orderBy('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with hive context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row \n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType\n",
    "from pyspark import HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "hivectx = HiveContext(ss.sparkContext)\n",
    "#hivectx.setConf(\"hive.metastore.uris\",\"thrift://localhost:10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(ID,IntegerType,true),StructField(name,StringType,true),StructField(age,IntegerType,true),StructField(numOfFriends,IntegerType,true)))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# way to pass schema\n",
    "input_data = spark.read.csv('./datasets/fakefriends.csv', \n",
    "                            schema = 'ID INT, name String, age INT, numOfFriends INT')\n",
    "# -- or --\n",
    "\n",
    "schema_friends = StructType([\n",
    "    StructField(name='ID',dataType=IntegerType(),nullable=False,metadata={\"info\":\"unique ID\"}),\n",
    "    StructField(name='name',dataType=StringType(),nullable=False,metadata={\"info\":\"name\"}),\n",
    "    StructField(name='age',dataType=IntegerType(),nullable=True,metadata={\"info\":\"age\"}),\n",
    "    StructField(name='numOfFriends',dataType=IntegerType(),nullable=True,metadata={\"info\":\"count\"})                           \n",
    "])\n",
    "input_data = spark.read.csv('./datasets/fakefriends.csv', \n",
    "                            schema = schema_friends\n",
    "                            )\n",
    "input_data.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* can infer schema if has column names\n",
    "* to infer schema, spark has to read all values which is data intensive, so dont infer and provide schema explicitly\n",
    "\n",
    "`input_data = spark.read.csv('./datasets/fakefriends.csv', inferSchema=True )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.createOrReplaceTempView('friends_fake')\n",
    "input_data.schema\n",
    "input_data.count()\n",
    "#hivectx.cacheTable('friends_fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------------+\n",
      "| ID|   name|age|numOfFriends|\n",
      "+---+-------+---+------------+\n",
      "| 21|  Miles| 19|         268|\n",
      "| 52|Beverly| 19|         269|\n",
      "| 54|  Brunt| 19|           5|\n",
      "|106|Beverly| 18|         499|\n",
      "|115|  Dukat| 18|         397|\n",
      "|133|  Quark| 19|         265|\n",
      "|136|   Will| 19|         335|\n",
      "|225|   Elim| 19|         106|\n",
      "|304|   Will| 19|         404|\n",
      "|341|   Data| 18|         326|\n",
      "|366|  Keiko| 19|         119|\n",
      "|373|  Quark| 19|         272|\n",
      "|377|Beverly| 18|         418|\n",
      "|404| Kasidy| 18|          24|\n",
      "|409|    Nog| 19|         267|\n",
      "|439|   Data| 18|         417|\n",
      "|444|  Keiko| 18|         472|\n",
      "|492|  Dukat| 19|          36|\n",
      "|494| Kasidy| 18|         194|\n",
      "+---+-------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagers = hivectx.sql(\"select * from friends_fake where age >=13 and age <=19\")\n",
    "teenagers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
